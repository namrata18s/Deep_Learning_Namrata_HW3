{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe10c15-67db-4aa9-9593-f6b17a07a47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14cf9a8c-2eca-4f2d-a633-aff9ed66f520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/nsurve/.conda/envs/pytorch/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -illow (/home/nsurve/.conda/envs/pytorch/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/nsurve/.conda/envs/pytorch/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -illow (/home/nsurve/.conda/envs/pytorch/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/nsurve/.conda/envs/pytorch/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -illow (/home/nsurve/.conda/envs/pytorch/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/nsurve/.conda/envs/pytorch/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -illow (/home/nsurve/.conda/envs/pytorch/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/nsurve/.conda/envs/pytorch/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -illow (/home/nsurve/.conda/envs/pytorch/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/nsurve/.conda/envs/pytorch/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -illow (/home/nsurve/.conda/envs/pytorch/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mâœ“ All libraries imported successfully!\n",
      "PyTorch version: 2.4.1+cu121\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-PCIE-40GB\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELL 1: Install & Import Libraries\n",
    "# ========================================\n",
    "# Run this cell first (only once)\n",
    "!pip install transformers tqdm -q\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca1b1a26-2610-45b0-8d77-4b2d0c0d7dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Dataset class defined\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELL 2: Dataset Class\n",
    "# ========================================\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, data_file, tokenizer, max_len=384, mode='train'):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Load data\n",
    "        print(f\"Loading {data_file}...\")\n",
    "        with open(data_file, 'r') as f:\n",
    "            raw_data = json.load(f)\n",
    "        \n",
    "        self.data = raw_data['data'] if 'data' in raw_data else raw_data\n",
    "        self.samples = []\n",
    "        self.process_data()\n",
    "        print(f\"âœ“ Loaded {len(self.samples)} samples\")\n",
    "    \n",
    "    def process_data(self):\n",
    "        for article in self.data:\n",
    "            for paragraph_info in article['paragraphs']:\n",
    "                context = paragraph_info['context']\n",
    "                \n",
    "                for qa in paragraph_info['qas']:\n",
    "                    question = qa['question']\n",
    "                    qid = qa['id']\n",
    "                    \n",
    "                    if self.mode == 'train' and qa['answers']:\n",
    "                        answer = qa['answers'][0]\n",
    "                        answer_start = answer['answer_start']\n",
    "                        answer_text = answer['text']\n",
    "                        \n",
    "                        self.samples.append({\n",
    "                            'id': qid,\n",
    "                            'question': question,\n",
    "                            'context': context,\n",
    "                            'answer_start': answer_start,\n",
    "                            'answer_text': answer_text\n",
    "                        })\n",
    "                    else:\n",
    "                        self.samples.append({\n",
    "                            'id': qid,\n",
    "                            'question': question,\n",
    "                            'context': context\n",
    "                        })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            sample['question'],\n",
    "            sample['context'],\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'id': sample['id']\n",
    "        }\n",
    "        \n",
    "        # Add token_type_ids only if available (BERT has it, RoBERTa doesn't)\n",
    "        if 'token_type_ids' in encoding:\n",
    "            item['token_type_ids'] = encoding['token_type_ids'].squeeze()\n",
    "        \n",
    "        # Add answer positions for training\n",
    "        if self.mode == 'train':\n",
    "            # Find answer position in tokens\n",
    "            answer_encoding = self.tokenizer(\n",
    "                sample['question'],\n",
    "                sample['context'],\n",
    "                max_length=self.max_len,\n",
    "                truncation=True,\n",
    "                return_offsets_mapping=True\n",
    "            )\n",
    "            \n",
    "            offsets = answer_encoding['offset_mapping']\n",
    "            answer_start = sample['answer_start']\n",
    "            answer_end = answer_start + len(sample['answer_text'])\n",
    "            \n",
    "            # Find start and end token positions\n",
    "            start_position = 0\n",
    "            end_position = 0\n",
    "            \n",
    "            for i, (offset_start, offset_end) in enumerate(offsets):\n",
    "                if offset_start <= answer_start < offset_end:\n",
    "                    start_position = i\n",
    "                if offset_start < answer_end <= offset_end:\n",
    "                    end_position = i\n",
    "                    break\n",
    "            \n",
    "            item['start_position'] = torch.tensor(start_position)\n",
    "            item['end_position'] = torch.tensor(end_position)\n",
    "        \n",
    "        return item\n",
    "\n",
    "print(\"âœ“ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fad2ac10-60ca-4e78-a430-c8e11c657f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set! Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELL 3: Configuration\n",
    "# ========================================\n",
    "CONFIG = {\n",
    "    'model_name': 'roberta-base',  # Change to 'roberta-base' for better results\n",
    "    'train_file': 'spoken_train-v1.1.json',\n",
    "    'test_file': 'spoken_test-v1.1.json',\n",
    "    'batch_size': 16,  # Reduce to 8 if you get memory errors\n",
    "    'num_epochs': 3,\n",
    "    'learning_rate': 3e-5,\n",
    "    'max_len': 384,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Configuration set! Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9567c9de-18e5-4ba6-97c1-8d5f6321e8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsurve/.conda/envs/pytorch/lib/python3.8/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating datasets...\n",
      "Loading spoken_train-v1.1.json...\n",
      "âœ“ Loaded 37111 samples\n",
      "Loading spoken_test-v1.1.json...\n",
      "âœ“ Loaded 5351 samples\n",
      "\n",
      "âœ“ Ready to train!\n",
      "Training batches: 2320\n",
      "Test batches: 335\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELL 4: Load Model & Data\n",
    "# ========================================\n",
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(CONFIG['model_name'])\n",
    "model.to(device)\n",
    "\n",
    "print(\"\\nCreating datasets...\")\n",
    "train_dataset = QADataset(CONFIG['train_file'], tokenizer, CONFIG['max_len'], mode='train')\n",
    "test_dataset = QADataset(CONFIG['test_file'], tokenizer, CONFIG['max_len'], mode='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "print(f\"\\nâœ“ Ready to train!\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d341b389-415a-4c7d-b352-278243412ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Training function defined\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELL 5: Training Function\n",
    "# ========================================\n",
    "def train_model(model, train_loader, device, num_epochs, lr):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Move to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['start_position'].to(device)\n",
    "            end_positions = batch['end_position'].to(device)\n",
    "            \n",
    "            # Get token_type_ids only if available (for BERT)\n",
    "            token_type_ids = batch.get('token_type_ids')\n",
    "            if token_type_ids is not None:\n",
    "                token_type_ids = token_type_ids.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                start_positions=start_positions,\n",
    "                end_positions=end_positions\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'âœ“ Epoch {epoch+1} complete - Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ Training complete!\")\n",
    "    return model\n",
    "\n",
    "print(\"âœ“ Training function defined\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d90f69d-45d4-4eaf-aeb3-9d76830d8f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Prediction function defined\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELL 6: Prediction Function\n",
    "# ========================================\n",
    "def predict(model, test_loader, tokenizer, device):\n",
    "    model.eval()\n",
    "    predictions = {}\n",
    "    \n",
    "    print(\"Making predictions...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Predicting'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            ids = batch['id']\n",
    "            \n",
    "            # Get token_type_ids only if available (for BERT)\n",
    "            token_type_ids = batch.get('token_type_ids')\n",
    "            if token_type_ids is not None:\n",
    "                token_type_ids = token_type_ids.to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            \n",
    "            start_logits = outputs.start_logits\n",
    "            end_logits = outputs.end_logits\n",
    "            \n",
    "            for i in range(len(ids)):\n",
    "                # Get best start and end positions\n",
    "                start_idx = torch.argmax(start_logits[i]).item()\n",
    "                end_idx = torch.argmax(end_logits[i]).item()\n",
    "                \n",
    "                # Make sure end >= start\n",
    "                if end_idx < start_idx:\n",
    "                    end_idx = start_idx\n",
    "                \n",
    "                # Limit answer length\n",
    "                if end_idx - start_idx > 30:\n",
    "                    end_idx = start_idx + 30\n",
    "                \n",
    "                # Get answer tokens\n",
    "                answer_tokens = input_ids[i][start_idx:end_idx+1]\n",
    "                answer_text = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "                \n",
    "                predictions[ids[i]] = answer_text.strip() if answer_text.strip() else \"unknown\"\n",
    "    \n",
    "    print(f\"âœ“ Generated {len(predictions)} predictions\")\n",
    "    return predictions\n",
    "\n",
    "print(\"âœ“ Prediction function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86a7ada7-e7cd-414d-8878-5139b33b83fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsurve/.conda/envs/pytorch/lib/python3.8/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 3 epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d158fafd58943569f810b035e711fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3:   0%|          | 0/2320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Epoch 1 complete - Average Loss: 2.1688\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87103d62fc1d4fd0994ea17d6bdb6233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/3:   0%|          | 0/2320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Epoch 2 complete - Average Loss: 1.1783\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6194c0f6a24c288b94ecde6404eaa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/3:   0%|          | 0/2320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Epoch 3 complete - Average Loss: 0.8260\n",
      "\n",
      "ðŸŽ‰ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELL 7: RUN TRAINING\n",
    "# ========================================\n",
    "# This is the main training cell - run this to start training!\n",
    "trained_model = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    device,\n",
    "    num_epochs=CONFIG['num_epochs'],\n",
    "    lr=CONFIG['learning_rate']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "704365f0-43aa-4e7f-aa4c-d1596c443656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a1b28a2f19464c8fcb4936f6b59b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Generated 5351 predictions\n",
      "\n",
      "âœ… All done! predictions.json has been created.\n",
      "Total predictions: 5351\n",
      "\n",
      "First 5 predictions:\n",
      "1. 56be4db0acb8001400a502ec: denver broncos\n",
      "2. 56be4db0acb8001400a502ed: denver broncos\n",
      "3. 56be4db0acb8001400a502ee: levis stadium\n",
      "4. 56be4db0acb8001400a502ef: denver broncos\n",
      "5. 56be4db0acb8001400a502f0: roman numerals\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELL 8: MAKE PREDICTIONS\n",
    "# ========================================\n",
    "# Run this after training completes\n",
    "predictions = predict(trained_model, test_loader, tokenizer, device)\n",
    "\n",
    "# Save predictions\n",
    "with open('predictions.json', 'w') as f:\n",
    "    json.dump(predictions, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ… All done! predictions.json has been created.\")\n",
    "print(f\"Total predictions: {len(predictions)}\")\n",
    "\n",
    "# Show a few examples\n",
    "print(\"\\nFirst 5 predictions:\")\n",
    "for i, (qid, answer) in enumerate(list(predictions.items())[:5]):\n",
    "    print(f\"{i+1}. {qid}: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afb7431-2a5f-452b-8205-78ecf55f06e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
